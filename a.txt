math is brisk

to address:

arrow vector

As we'll see later the paralellizable the better

more linearly

what was the point of this little history exercise? Some of the methods aren't pedigocailly clear today: key takeaways are that AI is not obvious and very miraculous

pedagogically clear vs history

miraculousness of AI

weights vs parameters (interchangable but a weird taxonomy there - weights originally refer to ) - prob in 

links vs footnotes

systems vs models

mention neuron terminology for perceptron

perceptron training single vs networks specified - be more specific about bigger networks, rossenblatt experimented with creating networks, but no training rule

what is the thread

structure:

complexitity born out of simplicity - evolution analogy

transformer-based

multi-positional linear probe


linear-probing and steering
note-representations
rhythm

polysemanticity


Introduction
Intro to ML
MusicGen
linear-probing and steering - note representations - need a hypothesis
mechanistic approach with SAEs and maybe circuits3