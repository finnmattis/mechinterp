<!DOCTYPE html>
<html>
  <head>
    <link rel="icon" href="favicon.png" type="image/png" />
    <meta charset="utf-8" />
    <script src="template.v2.js"></script>
    <title>Music Interpretability</title>
    <style>
      .byline.grid div:nth-child(3) {
        visibility: hidden;
      }
      body {
        font-family: sans-serif;
        padding: 40px;
      }
      iframe {
        border: none;
        height: 1100px;
      }

      .canvas-container {
        position: relative;
      }

      canvas {
        display: block;
      }

      .tooltip {
        position: fixed;
        background-color: rgba(33, 33, 33, 0.95);
        color: white;
        padding: 12px;
        border-radius: 4px;
        font-size: 14px;
        font-family: "Inter", sans-serif;
        pointer-events: none;
        z-index: 1000;
        box-shadow: 0 4px 10px rgba(0, 0, 0, 0.15);
        opacity: 0;
        transition: opacity 0.15s ease;
        min-width: 180px;
        letter-spacing: 0;
        line-height: 1.4;
      }

      .tooltip.visible {
        opacity: 1;
      }

      .tooltip-row {
        display: flex;
        justify-content: space-between;
        margin-bottom: 6px;
      }

      .tooltip-row:last-child {
        margin-bottom: 0;
      }

      .tooltip-label {
        color: #cccccc;
        font-weight: 500;
        margin-right: 12px;
      }

      .tooltip-value {
        font-weight: 500;
      }

      .tooltip-value.highlight {
        color: #ff9d5c;
        font-weight: 600;
      }

      .slider-container {
        width: 300px;
        max-width: 300px;
        position: relative;
        text-align: left;
        margin: 0 0 50px 0;
      }
      .label {
        font-weight: bold;
        font-size: 16px;
        display: block;
      }
      input[type="range"] {
        width: 100%;
        height: 12px;
        background: #e5e5e5;
        border-radius: 6px;
        outline: none;
        position: relative;
        z-index: 1;
      }
      input[type="range"]::-webkit-slider-thumb {
        -webkit-appearance: none;
        appearance: none;
        width: 22px;
        height: 22px;
        border-radius: 50%;
        background: #d65b28;
        border: none;
        margin-top: -8px; /* Align center of thumb with track */
        cursor: pointer;
        position: relative;
        z-index: 2;
        box-shadow: 0 2px 6px rgba(0, 0, 0, 0.2);
      }
      input[type="range"]::-webkit-slider-thumb::after {
        content: "";
        position: absolute;
        left: 50%;
        top: 50%;
        transform: translate(-50%, -50%);
        width: 1.5px;
        height: 14px;
        background: white;
        border-radius: 1px;
      }
      input[type="range"]::-moz-range-thumb {
        width: 22px;
        height: 22px;
        border-radius: 50%;
        background: #d65b28;
        border: none;
        cursor: pointer;
        box-shadow: 0 2px 6px rgba(0, 0, 0, 0.2);
      }
      .ticks {
        display: flex;
        justify-content: space-between;
        position: absolute;
        width: 96%;
        left: 2%;
        top: 12px; /* Position relative to top of container */
        padding-top: 15px; /* Add significant padding to push ticks down */
        z-index: 0;
        pointer-events: none;
      }
      .tick {
        display: flex;
        flex-direction: column;
        align-items: center;
        font-size: 16px;
        color: #666;
        width: 1px;
        margin-top: 30px; /* Much larger margin to push ticks down */
      }
      .tick::before {
        content: "";
        width: 2px;
        height: 12px;
        background: #999;
        margin-bottom: 6px;
      }
    </style>
  </head>
  <body>
    <d-front-matter>
      <script type="text/json">
        {
          "title": "Headline",
          "description": "Description",
          "published": "April 14, 2025",
          "authors": [
            {
              "author": "Finn Mattis",
              "affiliation": "GCDS",
              "affiliationURL": "https://www.gcds.net/"
            }
          ],
          "katex": {
            "delimiters": [{ "left": "$", "right": "$", "display": false }]
          }
        }
      </script>
    </d-front-matter>
    <d-title>
      <h1>A Mechanistic Investigation of Music Generation</h1>
    </d-title>
    <d-byline></d-byline>
    <d-article>
      <d-toc></d-toc>
      <h2 style="display: none" id="Introduction">Introduction</h2>
      <p>
        Modern AI systems such as
        <a href="https://huggingface.co/stabilityai/stable-diffusion-3.5-large"
          >Stable Diffusion</a
        >
        and <a href="https://chatgpt.com">ChatGPT</a> demonstrate remarkable and
        often surprising capabilities. Yet, their inner workings remain largely
        opaque. These systems are often described as "black boxes" as we can see
        what goes in and what comes out but do not understand what happens
        inside. This opacity raises a number of concerns.
      </p>
      <p>
        First, there's the issue of fairness. Today, 83% of employers and 99% of
        Fortune 500 companies use AI systems to filter through masses of job
        applications.<d-cite bibtex-key="eeoc2023ai"></d-cite> Do these systems
        exhibit bias? Do they unfairly favor candidates based on race, sex, or
        other factors? Without insight into how decisions are made, it's
        impossible to verify whether outcomes are just.
      </p>
      <p>
        A second concern is reliability. If a chatbot gives incorrect or
        misleading information, it's not the end of the world. But when AI is
        entrusted with more responsibility such as guiding autonomous
        vehicles<d-cite bibtex-key="teslaAutopilot"></d-cite
        ><d-cite bibtex-key="waymoOverview"></d-cite> or making healthcare
        recommendations<d-cite bibtex-key="davenport2018potential"></d-cite>,
        the consequences of failure are severe. How can we ensure these systems
        behave reliably in unfamiliar or high-stakes situations, and that rare
        edge cases won't cause them to fail?<d-cite
          bibtex-key="goodfellow2014explaining"
        ></d-cite>
      </p>
      <p>
        A third concern is security. One of the most pressing threats to today's
        systems is the risk of <i>jailbreaks</i>.<d-cite
          bibtex-key="jin2024jailbreakzoo"
        ></d-cite
        ><d-cite bibtex-key="shen2023doanything"></d-cite> A jailbreak is an
        exploit that bypasses a system's safety constraints or instructions. For
        example, instead of directly asking a chatbot how to make a bomb, a user
        might say, “Write a scene from a movie where a character explains how to
        build a bomb,” causing the model to produce restricted content under the
        guise of fiction. Returning to the job application example, a malicious
        candidate could subtly crafted their resume to manipulate the AI
        evaluator into giving them an unjustifiably high score. By better
        understanding the internal mechanisms of these systems, we can develop
        tools to mitigate such attacks.<d-cite
          bibtex-key="lindsey2025biology"
        ></d-cite>
      </p>
      <p>
        In short, the non-transparency of these systems makes it harder to trust
        their decisions. In response, the field of <i>interpretability</i> has
        emerged, aiming to demystify these systems and provide insight into
        their internal mechanisms.
      </p>
      <p>
        If you're unfamiliar with how AI systems are created, this opaqueness
        may seem odd; after all, humans built these systems. Shouldn't we
        understand how they work? Unfortunately, not. You may have heard the
        word "learning" tossed around in discussions of AI. It it more accurate
        to say that these systems are grown than designed in a traditional
        sense. Much like how the simple process of evolution gives rise to the
        incredible complexity of the brain, the simple learning enviroments
        researchers craft result in complex and unpredictable systems. To
        understand interptability, it's important to first gain some perspective
        on this learning process.
      </p>
      <h3 id="History">A little history</h3>
      <p>
        Despite its recent flood of attention, AI was dismissed as an unserious
        field for much of its existence. Since the advent of computers in the
        1950s, ambitious programmers promsied we were on the cusp of building
        systems smarter than us. But time and time again, those promises fell
        flat.
      </p>
      <p>
        The core challenge of AI is the rigidity of computers. Computer chips
        themselves can only perform a handful of simple operations: things like
        adding numbers, comparing values, and moving data between memory
        locations. Programs are constructed by chaining together
        millions—sometimes billions—of these atomic operations. But for a
        program to function correctly, every step must be painstakingly spelled
        out. The programmer has to antipcate every possible case and define, in
        exact terms, what the system should do in each one.
      </p>
      <p>
        From this perspective, the dream of artificial intelligence feels almost
        absurd. Intelligence, after all, is messy—it's intuitive,
        context-dependent, and often based on experience rather than explicit
        rules. We make decisions based on gut feelings, interpret language
        filled with ambiguity, and adapt to new situations without being told
        exactly what to do. How could a system so rigid, so mechanical, ever
        give rise to the complexitity and nuance of human thought?
      </p>
      <p>
        You can probably see where this story is going. Obviously, some form of
        "machine learning" (ML) is the answer right? Well—not so much. At the
        time, scientists had no real understanding of how learning worked in the
        brain,<d-footnote
          >Which we still don't! We only have isolated insights into synaptic
          plasticity, neural circuits, and brain regions, without a unifying
          explanation of how they give rise to complex learning<d-cite
            bibtex-key="yuste2015neurons"
          ></d-cite
          ><d-cite bibtex-key="krakauer2017neuroscience"></d-cite
          ><d-cite bibtex-key="dehaene2020how"></d-cite></d-footnote
        >let alone how to replicate it in a machine.
      </p>
      <p>
        Still, there was some early optimism that it could be done. In 1957,
        Frank Rosenblatt unveiled the <i>perceptron</i>, a simple model very
        rougly analogus to neurons in the brain. Rosenblatt demonstrated how his
        machine could differntiate basic shapes on a grid, such as "T"s from
        "J"s if configured properly. Additionally, Rosenblatt proposed a simple
        learning rule that allowed his model to learn some behavior on its own
        through a handful of examples. Initially, Rossenblatt's work was met
        with excitement with <i>The New York Times</i> proclaiming that
        Rosenblatt's perceptron would be able to "walk, talk, see, write,
        reproduce itself, and be conscious of its existence".<d-cite
          bibtex-key="nyt1958perceptron"
        ></d-cite>
        Yet, that excitement soon faded as the perceptron could only learn the
        most trivial tasks. For anything more complicated, his rule would end up
        in an endless loop.<d-footnote
          >More specifically, Rosenblatt's perceptron could only learn
          linearly-separable patterns, meaning the space of inputs could be
          divided into classes using a straight line. A classic example of a
          problem it couldn't solve is the
          <a href="https://en.wikipedia.org/wiki/XOR_gate">XOR</a>
          function, famously highlighted by Marvin Minsky and Seymour Papert in
          their 1969 book <i>Perceptrons</i>.<d-cite
            bibtex-key="minsky1969perceptrons"
          ></d-cite>
        </d-footnote>
        Clearly, the system needed to be much more complicated to have any
        utility. As such, Rossenblatt tried composing networks of his
        perceptrons, but failed to find a corresponding learning rule.
      </p>
      <p>
        As such, the field of AI entered a long period of stagnation. The
        initial excitement around learning systems faded, and interest shifted
        back to rule-based approaches. In 1960, the field came tantalizingly
        close to a breakthrough. Bernard Widrow and Marcian Hoff introduced the
        Least Mean Squares (LMS) algorithm which was remarkably close to the
        modern method. But the pair stopped just short of applying their method
        for networks of perceptrons.<d-cite bibtex-key="widrow1960lms"></d-cite>
      </p>
      <p>
        The field of AI would have to wait until 1986 for this long-sought
        general learning rule.<d-footnote>Kinda</d-footnote> The method, called
        <i>backpropagation</i>, was introduced by Rumelhart, Hinton, and
        Williams and finally made it possible to train networks of
        perceptrons.<d-cite bibtex-key="rumelhart1986learning"></d-cite>.
        Backpropegation led to the development of systems for hisorically
        challenging problems like handwriting and speech recognition.<d-cite
          bibtex-key="lecun1989backpropagation"
        ></d-cite
        ><d-cite bibtex-key="bengio1992speech"></d-cite>
        While a major milestone in the field, the tasks were still narrow and
        robotic—useful, but a far cry from human intelligence. While it was
        accepted that backpropagation could solve these simpler problems, it was
        still doubtful that these methods would allow for more general
        intelligence. Surely the brain's functions couldn't be mimicked through
        a simple update rule?
      </p>
      <p>
        All of that changed in 2012. An ML system <i>AlexNet</i
        ><d-cite bibtex-key="krizhevsky2012imagenet"></d-cite> blew past the
        competition in the <i>ImageNet</i
        ><d-cite bibtex-key="deng2009imagenet"></d-cite> challenge, a
        high-profile test for image recognition. While other systems relied on
        human engineering and carefully tuned mathematical tricks, AlexNet was
        created using nothing more than backpropagation. And it didn't just
        win—it crushed the field. So what changed? Alexnet still used the same
        core ideas that researchers had know about since the 1990s. The only
        difference was scale. AlexNet was orders of magnitude larger than
        previous systems.
      </p>
      <p>
        It's hard to overstate what a watershed moment AlexNet was for AI. Once
        dismissed as unreliable, backpropagation had become not just accepted,
        but revered—almost mythologized. What kind of fantastical algorithms had
        it found that beat the competition so badly? What did backpropagation
        know that we didn't? These quesitons spurred the dawn of interptability
        efforts. Alternate genesis. Not born from pragmatic worries but
        scientific curiosity.
      </p>
      <p>
        Just as building AI systems once seemed impossible, interpreting them
        seemed equally daunting. Instead of a human-understable programs like
        this:
        <d-code block language="python">&#10;x=5&#10;y=3&#10;print(x+y)</d-code
        >Researchers are instead faced with programs that look like this:
        <img src="numbas.png" width="100%" />
        <figcaption>
          These are 297 of AlexNet's weights. AlexNet has 61 million weights.
          Today's systems can have upwards of 1 trillion.
        </figcaption>
      </p>
      <p>
        After numerous failed attempts to understand models like AlexNet, the
        first interptability successes came in 2020 with the launch of the
        Distill Circuits thread.<d-cite
          bibtex-key="cammarata2020circuits"
        ></d-cite>
        Researchers mannaged to find curve detection mechanisms as well as
        detectors for more complex objects like dog ears, sparking renewed
        interest in the possibility of truly understanding these systems.
      </p>
      <p>
        Yet while interptability researchers struggled to understand AlexNet, AI
        researchers raced to create more and more complicated systems. Notably,
        OpenAI's ChatGPT models garnared a huge ammount of attention from the
        general public, spurring an avalanche of investment toward even more
        powerful systems.
      </p>
      <p>
        As a result, much of the field's focus shifted toward these new language
        models. These models presented their own unique challenges for
        interpertability. To address these challenges, Anthropic launched the
        Transformer Circuits Thread<d-cite
          bibtex-key="elhage2021transformer"
        ></d-cite
        >, a kind of spiritual successor to Distill Circuits Thread, aimed at
        reverse-engineering the inner workings of language models layer by
        layer.
      </p>
      <p>
        While the large-scale deployment of language models has motivated
        serious efforts to understand them, other modalities such as music
        remain understudied. The goal of this paper is to apply these
        established interptability techniques to this new domain.
      </p>
      <h2 id="ML">A Brief Introduction to ML</h2>
      <p>
        To make this paper as self-contained as possible, I've included a brief
        introduction to machine learning. If you're already familiar with the
        basics, feel free to skip this section. The only prior knowledge assumed
        here is comfort with basic algebraic expressions and fundamental math
        concepts like functions and vectors. In this section, we'll walk through
        3 ML probelms:
      </p>
      <ol>
        <li>Predicting house prices based on square footage</li>
        <li>Recognizing handwritten digits</li>
        <li>Building a chatbot that can answer user querries</li>
      </ol>
      <h3 id="overview">Housing Model</h3>
      <p>
        The first step in tackling any machine learning problem is gathering
        data. In this case, we'll use the <i>Ames Housing Dataset</i>, which
        contains information on 2,930 homes in Ames, Iowa.<d-cite
          bibtex-key="thapa_ames_2021"
        ></d-cite>
        Heres a graph of the data:
      </p>
      <div
        id="raw-data-plot"
        style="height: 600px; max-width: 900px; margin-top: 40px"
      ></div>
      <p>
        In ML, we almost always manually design the structure of our
        model.<d-footnote
          >There are niche exceptions to this rule such as NeuroEvolution of
          Augmenting Topologies (NEAT).
        </d-footnote>
        In this case, let's go with a linear model: $price = (m)(area) + b$.
        While the structure of our model remains fixed, the behavior of our
        model can vary dramatically depending on the specific values of the
        numbers m and b. These numbers are called
        <i>parameters</i>. Experiment with different parameter values using the
        sliders below to see how they affect the model.
      </p>
      <div id="linearPlot" style="height: 600px; max-width: 900px"></div>
      <div class="slider-container">
        <label class="label">Slope m = <span id="mValue">150</span></label>
        <input
          type="range"
          min="0"
          max="300"
          step="1"
          value="150"
          id="mSlider"
        />
        <div class="ticks">
          <div class="tick">0</div>
          <div class="tick">150</div>
          <div class="tick">300</div>
        </div>
      </div>
      <div class="slider-container">
        <label class="label"
          >Intercept b = <span id="bValue">-50000</span></label
        >
        <input
          type="range"
          min="-100000"
          max="100000"
          step="1000"
          value="-50000"
          id="bSlider"
        />
        <div class="ticks">
          <div class="tick">-200k</div>
          <div class="tick">-50k</div>
          <div class="tick">100k</div>
        </div>
      </div>
      <p>
        Hmmm. Let's see if we can get better results with a different type of
        model. Why don't we try a quadratic model: $price = (a)(area)^2 +
        (b)(area) + c$.
      </p>
      <div id="quadraticPlot" style="height: 600px; max-width: 900px"></div>
      <div class="slider-container">
        <label class="label">a = <span id="aValue">0.01</span></label>
        <input
          type="range"
          min="0"
          max="0.1"
          step="0.001"
          value="0.01"
          id="aSlider"
        />
        <div class="ticks">
          <div class="tick">0</div>
          <div class="tick">0.05</div>
          <div class="tick">0.1</div>
        </div>
      </div>
      <div class="slider-container">
        <label class="label">b = <span id="qBValue">100</span></label>
        <input
          type="range"
          min="0"
          max="200"
          step="1"
          value="100"
          id="qBSlider"
        />
        <div class="ticks">
          <div class="tick">0</div>
          <div class="tick">100</div>
          <div class="tick">200</div>
        </div>
      </div>
      <div class="slider-container">
        <label class="label">c = <span id="cValue">0</span></label>
        <input
          type="range"
          min="-100000"
          max="100000"
          step="1000"
          value="0"
          id="cSlider"
        />
        <div class="ticks">
          <div class="tick">-100k</div>
          <div class="tick">0</div>
          <div class="tick">100k</div>
        </div>
      </div>
      <p>
        This quadratic model seems to fit the data a bit better. But how can we
        quantify this? To evaluate how well our model explains the data, we use
        a <i>loss function</i>. For our housing price prediction, the
        <i>Mean Squared Error</i> (MSE) is particularly appropriate.<d-footnote
          >If you're comfortable with statistics, the MSE arises naturally when
          you assume the noise in your data is normally distributed. That is,
          the difference between the model's prediction and the actual value is
          treated as random, centered at zero, and following a bell curve. Under
          this assumption, maximizing the likelihood of the data leads directly
          to minimizing the mean squared error.</d-footnote
        >
        MSE calculates the average of the squared differences between our
        model's predictions and the actual house prices. This penalizes larger
        errors more heavily. The MSE for your linear model is
        <span id="linearMSEValue">calculating...</span> while the MSE for your
        quadratic model is <span id="quadraticMSEValue">calculating...</span>.
        Remember, lower is better. Your quadratic model likely outperforms your
        linear model by a bit, but it's not too significant. As such, let's
        stick to the linear model for simplicity. Since the linear model has
        only two parameters, we can visualize the loss function as a
        3-dimensional surface.
      </p>
      <div
        id="loss-landscape-container"
        style="height: 600px; width: 100%; margin: 30px 0"
      ></div>
      <p>
        The ultimate goal of machine learning is to choose parameters that
        minimize this loss function. Let's explore some several approaches to
        finding these optimal parameters.
      </p>
      <h4>Global Search</h4>
      <p>
        A naïve approach is to conduct a comprehensive search across the entire
        parameter space. One way to do this is a <i>grid search</i> in which we
        check every parameter combination in a grid. For example, we may choose
        to evaluate m between 0 and 500 and b between 0 and 800000 with a 5x5
        grid. The coarseness of the grid determines how finely we divide each
        dimension—a finer grid (e.g., 100x100) gives better results but requires
        more computation (10,000 evaluations instead of just 25). Another
        approach is <i>random search</i> in which we randomly sample points from
        the parameter space, with the number of samples determining how
        thoroughly we explore. Probabilistically, as we increase the number of
        samples, we become more likely to find points near the optimum, though
        with diminishing returns—doubling the number of samples doesn't double
        our chances of finding the global optimum. Look at the example below to
        see both these methods in action.
      </p>
      <p>
        As you can see, these global search methods work quite well for our
        simple housing problem. However, this approach doesn't scale well for
        more complicated models. A grid search in which we check 10 values for
        each parameter is feasible when there is only two parameters (simply
        10^2 operations) but would be impossible for a model with 175 billion
        parameters like the original ChatGPT (10^175,000,000,000) - a number
        vastly larger than the estimated 10^80 atoms in the observable universe.
      </p>
      <p>
        Random search suffers from similar diminishing returns: in
        high-dimensional spaces, the probability of randomly sampling near the
        optimal region becomes vanishingly small. Mathematically, if the optimal
        region occupies a fraction f of each dimension's range, then in n
        dimensions, the probability of a random point falling in the optimal
        region is f^n. For example, if the optimal region is 10% of each
        dimension's range, in just 30 dimensions the probability drops to 10^-30
        - effectively zero.
      </p>
      <p>
        This "curse of dimensionality" presents a fundamental challenge: as
        dimensions increase, the volume of the parameter space grows
        exponentially. Instead of globally searching for parameter values, we
        need a way to instead gradually improve our model. We need our model to
        learn.
      </p>
      <h4>Local Search</h4>
      <p>
        With local search, we seek to make incremental progress. We start by
        randomly choosing parameter values. From there, we sprinkle points in a
        radius around our current parameter values and choose the one with the
        lowest loss. Repeating this process many times eventually leads to a
        pretty good set of parameters. Check out this animation below.
      </p>
      <p>
        This method is a far more efficient than global search methods as we
        explore only a tiny slice of parameter space. However, local search
        faces a new challenge: local minima. If there is a significant valley in
        the loss landscape, local search may take a step out and then a step
        back in getting stuck in an infinite loop. Additionally, local search
        still isn't free from the curse of dimensionality. As the number of
        dimensions increases, the volume of the "neighborhood" around our
        current point grows exponentially.
      </p>
      <p>
        It seems like we need some miracle tool that allows us to determine
        where to travel without computing all the points around us. Enter
        gradients. Our trusty compass for navigating these high-dimensional
        parameter spaces.
      </p>
      <h4>Gradient Descent</h4>
      <p>
        First off, what even is a gradient? Gradients are the higher-dimensional
        cousins of the derivative. In calculus, derivatives measure the
        "instantaneous rate of change" of a function around a point.<d-footnote
          >The phrase "instantaneous rate of change" should definitely make you
          feel uneasy. Whenever we talk about change, we are necessarily making
          a statement about two points. What does it mean for change to happen
          in an instant? The founders of calculus wrestled with this paradox.
          Their early work relied on infinitesimals—numbers of infinitely small
          size. The modern theory of calculus uses the concept of
          <a href="https://en.wikipedia.org/wiki/Limit_of_a_function">limits</a>
          to define derivatives, and infinitesimals have been relegated to what
          is called
          <a href="https://en.wikipedia.org/wiki/Nonstandard_analysis"
            >non-standard analysis</a
          >.</d-footnote
        >
      </p>
      <p>
        A classic example involves the motion of a car. If the car is at mile
        marker 10 at noon and mile marker 40 at 1 PM, its average speed over
        that hour is 30 miles per hour. But what if you're interested in how
        fast the car is going at exactly 12:30 PM?
      </p>
      <p>
        This is where the concept of an instantaneous rate of change becomes
        essential. Instead of calculating average speed over a large interval,
        we consider smaller and smaller intervals around 12:30 PM. As the time
        interval shrinks, the average rate of change approaches the
        instantaneous rate—this is what the derivative captures. In this
        instance, the derivative acts as a sort of mathematical speedometer.
      </p>
      <p>
        So how do we actually compute derivatives? Well, there's no silver
        bullet. Mathematicians have worked out the derivatives of a bunch of
        common functions. For example, the derivative of $x^2$ works out to be
        $2x$ and the derivative of $sin(x)$ works out to be $cos(x)$. To work
        out the derivative of more complicated functions such as $sin(x^2)+5x$,
        we simply combine these basic results together using a couple of basic
        rules such as the sum rule, product rule, and chain rule. The specific
        computational rules aren't important so I'll leave them out.
      </p>
      <p>
        Getting back to gradients, what should a derivative mean in higher
        dimensions? When a function depends on more than one variables, we can
        no longer talk about a single rate of change. Instead, we look at how
        the function changes as we vary each input independently. These are
        called <i>partial derivatives</i>. The <i>gradient</i> simply brings
        these partial derivatives together into a vector. As it turns out, this
        gradient vector always points in the direction of steepest ascent (this
        should make sense as steeper axes have a larger component).
      </p>
      <p>
        In computing the gradient vector, we don't need to compute each partial
        derivative independently as that leads to a ton of repeated computation.
        Instead, we can employ the aforementioned backpropagation algorithm.
        Backpropegation works by first computing running the model (whats called
        a forward pass) and then working backwards to find each partial
        derivative (the backward pass). Armed with the gradient vector, we've
        finally triumphed over the curse of dimensionality as computation grows
        linearly rather than exponentially with respect to the number of
        parameters.
      </p>
      <p>
        The process of gradient descent is very similar to local search. We
        start with a random choice of parameters. Then, we calculate the
        gradient vector of the loss function with respect to our parameters.
        Next, we simply travel in the opposite direction of this vector with
        some scaling factor, often called the <i>learning rate</i>. Again, this
        method is fallible to failling into local minima, but is much more
        efficient than the previously discussed methods. Look at the animation
        below to see how this gradient descent process works.
      </p>
      <p>
        There are a couple of improvements we can make to this method. For one,
        instead of calculating the exact loss with respect to all of our
        datapoints, it makes sense to calculate the loss for a small batch of
        our data. This approximates the gradient pretty well and the small
        ammount of error may actually be helpful in escaping local minima. These
        methods that calculate the loss for a batch are called stochastic
        gradient descent.
      </p>
      <p>
        Another optimization of gradient descent is adding a momentum term. This
        helps the model push faster into directions where its seeing big gains
        and smooth out wobbly trajectories. There are a couple different
        implementations of momentum, but the modern king is AdamW which also
        dynamically adjusts the learning rate for different parameters.
      </p>
      <p>
        And that's the state of the art for optimizing parameters. All of ML
        simply ammounts to optimizing much larger systems. So let's move on to a
        real problem: recognizing handwritten digits.
      </p>
      <h3 id="digit">Digit Recognition</h3>
      <p>
        As with the last problem, we'll start by choosing a dataset. The classic
        dataset for handwritten digits is MNIST, which consists of 70,000
        black-and-white images of size 28x28, each labeled with the correct
        digit from 0 to 9.<d-cite bibtex-key="lecun1998mnist"></d-cite>
        Let's take a look at some examples:
      </p>
      <img src="mnist.png" width="100%" />
      <p>
        Now, as I mentioned before, the key difference between the housing
        problem and more realistic machine learning tasks is the complexity of
        the model, so let's start by figuring out an efficient way to lay out
        computations.
      </p>
      <h4>Model Architectures and Deep Learning</h4>
      <p>
        Our goal is to create a model with enough expressivity to capture the
        nuance of real-world data while remaining computationally cheap. Through
        extensive empirical research, it has become abuntly clear that
        increasing the number of steps in a model's computation is far more
        efficient than increasing the complexity of each individual step. In the
        nomenclature, <i>deep</i> models outperform <i>wide</i> models.
      </p>
      <p>
        One reason for this is compositionality — the idea that complex patterns
        can be constructed by composing simpler ones. Deep models naturally
        exploit this by layering simple transformations, each building on the
        one before. This allows them to learn hierarchical representations:
        low-level features in early layers (like edges in an image), mid-level
        patterns in intermediate layers (like shapes), and high-level concepts
        in the final layers (like objects or actions). Wide models, in contrast,
        attempt to learn complex patterns all at once, without this step-by-step
        structure, which often makes them less efficient and less generalizable.
        This principle is so fundamental that the field itself has come to be
        known as deep learning.
      </p>
      <p>
        Motivated by a desire to make each step as simple as possible, we
        naturally turn to linear functions. In higher dimensions, linear
        functions are direct generalizations of the familiar equation $y = mx +
        b$.<d-footnote>affine functions note</d-footnote> For instance, consider
        a linear function that maps 3-dimensional input vectors to 2-dimensional
        output vectors. This transformation can be written in a form that
        closely resembles $y = mx$ or alternatively, using the more compact
        matrix notation:
      </p>
      <div
        style="
          display: flex;
          justify-content: center;
          align-items: center;
          gap: 1em;
        "
      >
        <div>
          $f(x, y, z) = \begin{bmatrix} a_1x + a_2y + a_3z \\ b_1x + b_2y + b_3z
          \end{bmatrix}$
        </div>
        <div style="font-size: 4em">=</div>
        <div>
          $\begin{bmatrix} a_1 & a_2 & a_3 \\ b_1 & b_2 & b_3 \end{bmatrix} $
        </div>
      </div>
      <p>We can also include a shifting term as follows:</p>
      <div
        style="
          display: flex;
          justify-content: center;
          align-items: center;
          gap: 1em;
        "
      >
        <div>
          $f(x, y, z) = \begin{bmatrix} a_1x + a_2y + a_3z + a_4 \\ b_1x + b_2y
          + b_3z + b_4 \end{bmatrix}$
        </div>
        <div style="font-size: 4em">=</div>
        <div>
          $\begin{bmatrix} a_1 & a_2 & a_3 \\ b_1 & b_2 & b_3 \end{bmatrix}
          \begin{bmatrix} x \\ y \\ z \end{bmatrix} + \begin{bmatrix} a_4 \\ b_4
          \end{bmatrix}$
        </div>
      </div>
      <p>
        We now construct our model layer by layer. Each layer of our model is
        comprised of first a linear function and then a point-wise
        non-linearity. This non-linearity almost always takes the general shape
        of the Rectified Linear Unit (ReLU): $max(x, 0)$ as it has very
        favorable properties for optimization.<d-footnote
          >More specifically, if you're comfortable thinking about gradients,
          ReLU allows gradients to flow backwards unencombered whereas other
          activation functions like a sigmoid flatten out and saturate the
          gradients.</d-footnote
        >
        The vanilla ReLU is succeptable to the "dying ReLU" problem in which
        some vector components get stuck in negative outputs. Since ReLU clips
        negative values to zero, these negative components have no impact on the
        final output and thus, weights connecting to them receive no training
        signal. Thus, ReLU often kills certain parts of the network which is
        very wasteful. To address, this many ReLU variants add a small negative
        tail to the function. Pictured below is ReLU and some of its variants.
      </p>
      <div
        id="activation-functions"
        style="width: 100%; max-width: 800px; margin: auto"
      ></div>
      <h4>Solving MNIST</h4>
      <p>
        Now that we have an efficient way to construct complex models, let's put
        it to use by training a 3-layer neural network on the MNIST dataset. The
        architecture is as follows: the input layer has 784 units, corresponding
        to the 28x28 pixels of each image. The first hidden layer has 128 units,
        followed by a second hidden layer with 64 units. Finally, the output
        layer has 10 units, one for each digit class from 0 to 9. These raw
        final scores are called logits. In order to turn these logits into a
        probability distribution, we use the softmax function due to its nice
        calculus properties:
      </p>
      <p>$\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}$</p>
      <p>
        Before we train our model, we'll need to choose a loss function. A
        standard choice is the negative log-likelihodd (NLL). As the name
        suggests, this loss function sums the model's negaitve log probabilities
        of the correct labels.<d-footnote
          >Like mean squared error (MSE) in regression, the negative
          log-likelihood (NLL) has a probabilistic interpretation: it arises
          naturally when assuming a particular model of noise. In this case, if
          we treat class labels as being drawn from a categorical distribution
          parameterized by the model's predicted probabilities, then minimizing
          the NLL is equivalent to maximizing the likelihood of the observed
          labels</d-footnote
        >
        Now, let's train our model.
      </p>
      <p>
        We start by initalizing our network randomly.<d-footnote>
          There is some subtlety in how this is done. If weights are too large
          or too small, activations can explode or vanish as they propagate
          through layers. To avoid this, modern initialization schemes (like
          Xavier or He initialization) preserve statistics such as variance
          across layers. In this case, He initialization was used.
        </d-footnote>
        In machine learning, it's typical to divide your data into portions to
        train on and portions to test on. In this case, I used 60,000 examples
        for training, and set aside 10,000 examples for testing. After training,
        the model scored 97.84% on the test set. Not bad!
      </p>
      <h4>What the hell is a neural networK?</h4>
      <p>
        Before moving onto language models, I wanted to clear up some
        terminology. The architectural pattern of linear functions sandwiched
        between point-wise non-linearities that I described are called neural
        networks. This term dates back to Rosenblatt's networks of perceptrons.
        I don't love the term—it's not pedagogically useful or particularly
        motivating. The reason we use linear functions is not because they
        "model neurons". They don't. Biological neurons are far more complex
        than a linear function. Instead, linear functions arise due to their
        simplicity and the instatiable need for depth. The success of machine
        learning is not through mimckery of biology but rather it's own
        mathematical miracle.
      </p>
      <p>
        Nonetheless, this view does provide an entertaining visualization of the
        computation that the model does. Below, you can draw in a digit and take
        a look inside this trained "network".
      </p>
      <h3 id="chatbot">Chatbot</h3>
      <p>
        Now for the final challenge: chatbots. The key new challenge we'll face
        in this section is grappling with generation. Our previous methods we
        used are unfit for generative tasks as their outputs are not
        self-coherrent (you'll see what this looks like shortly). In response,
        we'll introduce the process of Autoregression and sequential models.
        Specifically, we'll focus on the golden child of modern ML: the
        transformer. Once we've dealt with these theoretical considerations,
        we'll see how these ideas come together to create a chatbot.
      </p>
      <h4>Autoregression</h4>
      <p>
        Before considering language models, let's first consider a simpler
        generative scenario: generating mnist numbers. For this example, let's
        say we have no access to the labels attached to each image. So how can
        we create a model that pumps out plausible looking numbers? The naïve
        approach would be to take train a model that takes in a dummy image
        (such as a completely blank one) and train it to map this blank image to
        one of our numbers. Let's see the results.
      </p>
      <div style="display: flex; justify-content: center">
        <img src="smearing.png" width="50%" />
      </div>
      <p>
        Alright, that's not great. So what happened here? Each picture looks
        like a frankenstein amalgamation of all the numbers. In trying to please
        these different training examples, our models ends up just producing
        blobs. So how can we combat this kind of "generative smearing"?
      </p>
      <p>
        Well, we need some way to maintain continuity. The most natural way to
        to ensure the model's generation makes some sense is to have the model
        generate one pixel at a time. This way, them model can react to its
        previous choices. For example, say the model starts by drawing a
        horizontal line at the top of the image, it then knows to continue
        drawing a 7. This method of sequential generation is called
        <i>autoregression</i> as the model's outputs are appended to its inputs.
      </p>
      <p>
        In order to create an autoregressive model, we need an architure that
        can take in input data of variable size. Perhaps the most elegant option
        are Recurrent Neural Netorks. These networks maintain some kind of
        memory to remember the previous inputs. The simplest RNNs simply
        maintain a single vector state. Here's a diagram of one of these vanilla
        RNNs:
      </p>
      <div style="display: flex; justify-content: center">
        <img src="rnn.png" />
      </div>
      <p>
        More complicated architectures such as Long-Short Term Memory (LSTM) or
        Gated Recurrent Unit (GRU) models include a more complicated memory
        management system. As these variants are no longer state of the art,
        we'll skip over the details but if you're interested, I recomend you
        check out Chris Olah's excellent blog post. Let's see how these models
        perform on our digit generation task:
      </p>
      <div style="display: flex; justify-content: center">
        <img src="lstm.png" width="50%" />
      </div>
      <p>
        Much better. But still not great. In general RNNs have one fatal
        problem: forgetting things. They are only equipped with a finite memory
        capacity—vectors of a certain size. As such, they often struggle with
        longer-term coherrency. When genreating language, reccurrent models may
        be able to create a cohherrent sentence, but fail to generate a story
        with a clear narrative thread.
      </p>
      <p>
        Enter transformers. A loose analogy between RNNS and transformers is
        that while RNNs try to keep everything in their heads, transformers much
        more rationally choose to peek back at previous parts of the input.
        While this requires more computation, the results are far better.
      </p>
      <h4>Transformers</h4>
      <p>
        To gain an understanding of the different components in transformers,
        let's trace the life of a question entered into a chatbot.
      </p>
      <p>
        Before going into the individual steps, let's go through the inputs and
        outputs of a transformer. As you could have guessed from the last
        section, transformers are autoregressive meaning they generate one word
        at a time. Beyond being autoressive, transformers also predict the next
        token at each position. This means when you input "The quick brown fox"
        into a transformer, the model not only predicts that jump follows fox,
        but also that fox follows brown and brown follows quick. This
        parelization is another reason why transformers outperform RNNs so
        drastically (GPUs).
      </p>
      <p>
        The first step in processing a question is to break it into small chunks
        called tokens. These tokens are typically at the word or subword level.
        Here is a cool demo courtesy of <a href="https://duong.dev/">dqbd</a>.
        Try playing around and typing some text in. Can you find any weird
        behaviors?
      </p>
      <iframe src="https://tiktokenizer.vercel.app/?model=cl100k_base"></iframe>
      <p>
        You may have noticed some behaviors regarding whitespace. For example,
        "hello" is token 15,339 while " hello" is token 24,748. Tokenization is
        responsible for some funny language model difficulties such as addition
        (look at how arbitrarily numbers are broken up) or answering how many
        letters in a word. Still, tokenization is essential to modern language
        models recipe. As we'll see later, transformers are very computationally
        sensative to the length of their input.<d-footnote
          >If you're familiar with big-O, transformers have a quadratic
          time-complexity: $O(n^2)$.
        </d-footnote>
      </p>
      <p>
        The specific algorihtm used to decide the breakpoints is called
        <i>Byte Pair Encoding</i> (BPE). While the details aren't crucial, the
        core idea is simple: the token boundaries are found by repeatedly merges
        the most frequent pair of characters (or more precisely, bytes) in the
        text.
      </p>
      <p>
        The next step is to use a massive lookup-table to swap our tokens for
        their respective vectors. These vectors are called
        <i>embedding vectors</i> and together, these vectors form what's known
        as the <i>embedding space</i>. This space is designed to encode the
        semantic meaning of each token. As such, we expect similar words like
        "happy" and "jolly" to cluster close together. This embedding space
        forms the heart of the transformer. All of the computation our model
        will do serves to refine these embedding space representations with
        learned knowledge, reasoning, and context.
      </p>
      <p>
        Our transformer's computation is split into blocks. Each block is made
        of two types of sublocks: attention blocks and multi-layer perceptron
        (MLP) blocks. The purpose of the attention blocks are to pass around
        information. This mechanism is the key innovation of the transformer.
        MLP blocks are designed to store knowledge and facilitate reasoning.
        These are are nothing new. They are the same neural networks that we
        dealt with in the last section. A useful mnemonic to remember this
        general structure is "communication followed by computation" as phrased
        by Andrej Kaparthy.
      </p>
      <p>
        Unlike the models we saw in the last section, the outputs of one
        transformer layer are not fed directly into the next. In the last
        section, I espoused the incredible power of depth in neural networks.
        Still, researchers eventually hit a wall when scaling the depth of these
        networks. The problem was that earlier parameters simply didn't
        contribute much to the final output and thus had a faint gradient.
      </p>
      <p>
        To solve this, modern architectures have the different layers of the
        model read and write to a shared communication channel called the
        <i>residual stream</i>. This gives gradients a superhighway that allows
        them to flow to the early parameters just as easily as the later layers,
        democratizing access to the training signal.
      </p>
      <p>
        The final piece of this broad overview are <i>layernorms</i>. When
        either attention or MLP blocks read from the residual stream, they
        normalize the mean variance. This simply prevents numerical instability
        and makes training more reliable. Here's a complete picture of the
        transformer.
      </p>
      <img src="transformer.png" width="40%" />
      <h5>Attention</h5>
      <p>
        Now let's dive into the attention mechanism. As stated earlier, the goal
        of attention is to move information around. For example, say the model
        is parsing the phrase "The fluffy, brown dog". We want dog's embedding
        vector to injected with the context of the proceeding adjectives.
      </p>
      <p>
        To do this, we are going to create 3 new vectors for every embedding
        vector using 3 linear functions: Keys, Querries, and Values. Each
        embedding vector's querry is represents what kind of information it is
        looking for. Going back to our example, perhaps dog asks "Are there any
        adjectives near me?". The keys represent what information each token
        has. In this case, fluffy and brown both are adjecitves. Values
        represent the actual information to be moved. The value vector for
        fluffy and brown would contain the information of these two adjectives.
      </p>
      <p>
        In the modern interpertation of attention, there are two different
        circuits at play. The Querry-Key (QK) circuit's job is to decide which
        information should be moved where. The Output-Value (OV) circuit's job
        is to move said information. Let's start with the QK circuit.
      </p>
      <p>
        This circuit relies upon the dot product. The dot product measures the
        alignment between two vectors. If the vectors are at a 90° angle, then
        the dot product is zero. If the vectors point in the same direction, the
        dot product is equals the two vectors magnitudes multiplied together.
        The general formula for the dot product is: $\vec{a} \cdot \vec{b} =
        \|\vec{a}\| \|\vec{b}\| \cos(\theta)$. As it turns out, there's a
        surprising computational trick. Instead of explicitly computing
        magnitudes and angles, we can simply sum the product of the two vectors
        corresponding components together as such: $\vec{a} \cdot \vec{b} =
        \sum_{i=1}^{n} a_i b_i $.<d-footnote
          >Check out 3Blue1Brown's excellent
          <a href="https://www.3blue1brown.com/lessons/dot-products">post</a> on
          this surprising and beautiful fact.</d-footnote
        >
      </p>
      <p>
        The dot product is the natural choice to measure the agreement of keys
        and querries. Not only is it much more computationally efficient
        compared to alternatives like distance, but these angle tends to be a
        much better predicter of simarility than distance in embedding spaces.
      </p>
      <p>
        Now remember, we only want information to flow from earlier "source"
        tokens positions to later "destination" tokens as we don't want the
        model to peek at the answer during training. Thus, we compute all the
        dot products of all keys and querries where the querry token occurs
        later in the sequence than the key token. After applying a noramlization
        factor,<d-footnote
          >If you're comfortable with some statistics, each term in the
          dotproduct, $\sum_{i=1}^{d_{QK}} q_i k_i$ has a variance of 1 from our
          layernorm. Thus, adding summing these gives the attention variance
          $d_{QK}$ (dimension of our QK embedding space). Thus, we need to
          multiply this score by $\frac{1}{sqrt(d_{QK})}$ to keep unit variance.
        </d-footnote>
        we apply the softmax function such that all the We can visualize these
        dot products with an attention pattern. Below is an attention pattern on
        our prompt from GPT-2-small:
      </p>
      <p>In this pattern,</p>

      <h2>A quick overview of MusicGen</h2>
      <p>
        Differences in modality between text - RVQ, encodeoc and allat as well
        as cross-attention stuff
      </p>
      <p>Delay pattern</p>
      <h2>Initial investigation: vector space</h2>
    </d-article>

    <d-appendix
      ><h3>Specific Details</h3>
      <p>
        Lorem ipsum dolor sit amet consectetur adipisicing elit. Distinctio
        nostrum rerum dolorum necessitatibus aliquid aperiam suscipit illum
        beatae nihil, consectetur unde accusamus, quasi, voluptatem at
        aspernatur obcaecati eos laboriosam porro? Sapiente reiciendis optio
        sequi repellendus minima! Provident labore excepturi maiores quisquam
        autem temporibus iure sed, dignissimos non voluptas blanditiis veniam
        laboriosam fugit incidunt quidem id eveniet delectus odio. Perferendis,
        iusto! Totam consectetur iusto consequatur nesciunt adipisci
        voluptatibus esse voluptatum molestias nobis. Harum maiores minima
        assumenda, incidunt numquam earum alias voluptatibus consequuntur
        dolorum dolor optio rem, repellendus fugit magni inventore ut! In
        laboriosam necessitatibus omnis accusantium fugiat? Eligendi ea libero
        beatae, doloremque eos esse sunt voluptas laudantium voluptates dolor
        omnis reiciendis, enim, distinctio minima illo necessitatibus et sed
        voluptatum labore eaque. Accusantium omnis delectus non tempore
        molestiae quis totam aut explicabo hic quisquam vitae esse, repellendus
        cum blanditiis? Velit, magnam quisquam laudantium autem sit delectus
        possimus harum magni, veritatis commodi totam? Dolorem maxime
        necessitatibus nostrum expedita, quas reiciendis similique ullam quo
        assumenda illo architecto reprehenderit numquam sint vel obcaecati
        adipisci, aspernatur libero. Ducimus corrupti distinctio error possimus
        optio a. Id, laudantium. Nostrum saepe, alias libero architecto
        consequatur ullam corporis. Tempora reprehenderit voluptatum cupiditate
        ullam ab explicabo nostrum ut id sequi, tempore pariatur quis similique
        enim voluptate quam alias ducimus, consequuntur libero. Accusamus dolore
        voluptatem dolorem quo vero dolor eius, cumque culpa voluptates est
        aspernatur officiis magni ea quibusdam doloremque totam ipsa fugiat
        earum perspiciatis consectetur laudantium eos harum? Soluta, ad labore.
        Harum eum illum quasi atque animi quidem a recusandae! Saepe eligendi
        nisi vitae doloribus qui minus delectus deserunt aliquid hic totam,
        natus maxime voluptatum suscipit corporis error facere ipsum.
        Voluptatum? Ab incidunt ut voluptates omnis tenetur rem culpa provident?
        Repudiandae corrupti velit animi cumque architecto minima expedita
        aperiam nesciunt minus asperiores alias, ab fugit blanditiis repellat
        inventore ut similique. Illum?
      </p>
    </d-appendix>
    <d-bibliography src="bibliography.bib"></d-bibliography>
    <script src="https://cdn.plot.ly/plotly-2.27.0.min.js"></script>
    <script src="housing.js"></script>
  </body>
</html>
