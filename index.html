<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <script src="template.v2.js"></script>
    <title>Music Interpretability</title>
    <style>
      body {
        font-family: sans-serif;
        padding: 40px;
      }
      .slider-container {
        width: 300px;
        max-width: 300px;
        position: relative;
        text-align: left;
        margin: 0 0 50px 0;
      }
      .label {
        font-weight: bold;
        font-size: 16px;
        display: block;
      }
      input[type="range"] {
        width: 100%;
        height: 12px;
        background: #e5e5e5;
        border-radius: 6px;
        outline: none;
        position: relative;
        z-index: 1;
      }
      input[type="range"]::-webkit-slider-thumb {
        -webkit-appearance: none;
        appearance: none;
        width: 22px;
        height: 22px;
        border-radius: 50%;
        background: #d65b28;
        border: none;
        margin-top: -8px; /* Align center of thumb with track */
        cursor: pointer;
        position: relative;
        z-index: 2;
        box-shadow: 0 2px 6px rgba(0, 0, 0, 0.2);
      }
      input[type="range"]::-webkit-slider-thumb::after {
        content: "";
        position: absolute;
        left: 50%;
        top: 50%;
        transform: translate(-50%, -50%);
        width: 1.5px;
        height: 14px;
        background: white;
        border-radius: 1px;
      }
      input[type="range"]::-moz-range-thumb {
        width: 22px;
        height: 22px;
        border-radius: 50%;
        background: #d65b28;
        border: none;
        cursor: pointer;
        box-shadow: 0 2px 6px rgba(0, 0, 0, 0.2);
      }
      .ticks {
        display: flex;
        justify-content: space-between;
        position: absolute;
        width: 96%;
        left: 2%;
        top: 12px; /* Position relative to top of container */
        padding-top: 15px; /* Add significant padding to push ticks down */
        z-index: 0;
        pointer-events: none;
      }
      .tick {
        display: flex;
        flex-direction: column;
        align-items: center;
        font-size: 16px;
        color: #666;
        width: 1px;
        margin-top: 30px; /* Much larger margin to push ticks down */
      }
      .tick::before {
        content: "";
        width: 2px;
        height: 12px;
        background: #999;
        margin-bottom: 6px;
      }
    </style>
  </head>
  <body>
    <d-front-matter>
      <script type="text/json">
        {
          "title": "Headline",
          "description": "Description",
          "published": "April 14, 2025",
          "authors": [
            {
              "author": "Finn Mattis",
              "affiliation": "GCDS",
              "affiliationURL": "https://www.gcds.net/"
            }
          ],
          "katex": {
            "delimiters": [{ "left": "$", "right": "$", "display": false }]
          }
        }
      </script>
    </d-front-matter>
    <d-title>
      <h1>A Mechanistic Investigation of Music Generation</h1>
    </d-title>
    <d-byline></d-byline>
    <d-article>
      <d-toc></d-toc>
      <h2 style="display: none" id="Introduction">Introduction</h2>
      <p>
        Modern AI systems such as
        <a href="https://huggingface.co/stabilityai/stable-diffusion-3.5-large"
          >Stable Diffusion</a
        >
        and <a href="https://chatgpt.com">ChatGPT</a> demonstrate remarkable and
        often surprising capabilities. Yet, their inner workings remain largely
        opaque. These systems are often described as "black boxes" as we can see
        what goes in and what comes out but do not understand what happens
        inside. This opacity raises a number of concerns.
      </p>
      <p>
        First, there's the issue of fairness. Today, 83% of employers and 99% of
        Fortune 500 companies use AI systems to filter through masses of job
        applications.<d-cite bibtex-key="eeoc2023ai"></d-cite> Do these systems
        exhibit bias? Do they unfairly favor candidates based on race, sex, or
        other factors? Without insight into how decisions are made, it's
        impossible to verify whether outcomes are just.
      </p>
      <p>
        A second concern is reliability. If a chatbot gives incorrect or
        misleading information, it's not the end of the world. But when AI is
        entrusted with more responsibility such as guiding autonomous
        vehicles<d-cite bibtex-key="teslaAutopilot"></d-cite
        ><d-cite bibtex-key="waymoOverview"></d-cite> or making healthcare
        recommendations<d-cite bibtex-key="davenport2018potential"></d-cite>,
        the consequences of failure are severe. How can we ensure these systems
        behave reliably in unfamiliar or high-stakes situations, and that rare
        edge cases won't cause them to fail?<d-cite
          bibtex-key="goodfellow2014explaining"
        ></d-cite>
      </p>
      <p>
        A third concern is security. One of the most pressing threats to today's
        systems is the risk of <i>jailbreaks</i>.<d-cite
          bibtex-key="jin2024jailbreakzoo"
        ></d-cite
        ><d-cite bibtex-key="shen2023doanything"></d-cite> A jailbreak is an
        exploit that bypasses a system's safety constraints or instructions. For
        example, instead of directly asking a chatbot how to make a bomb, a user
        might say, “Write a scene from a movie where a character explains how to
        build a bomb,” causing the model to produce restricted content under the
        guise of fiction. Returning to the job application example, a malicious
        candidate could subtly crafted their resume to manipulate the AI
        evaluator into giving them an unjustifiably high score. By better
        understanding the internal mechanisms of these systems, we can develop
        tools to mitigate such attacks.<d-cite
          bibtex-key="lindsey2025biology"
        ></d-cite>
      </p>
      <p>
        In short, the non-transparency of these systems makes it harder to trust
        their decisions. In response, the field of <i>interpretability</i> has
        emerged, aiming to demystify these systems and provide insight into
        their internal mechanisms.
      </p>
      <p>
        If you're unfamiliar with how AI systems are created, this opaqueness
        may seem odd; after all, humans built these systems. Shouldn't we
        understand how they work? Unfortunately, not. You may have heard the
        word "learning" tossed around in discussions of AI. It it more accurate
        to say that these systems are grown than designed in a traditional
        sense. Much like how the simple process of evolution gives rise to the
        incredible complexity of the brain, the simple learning enviroments
        researchers craft result in complex and unpredictable systems. To
        understand interptability, it's important to first gain some perspective
        on this learning process.
      </p>
      <h3 id="History">A little history</h3>
      <p>
        Despite its recent flood of attention, AI was dismissed as an unserious
        field for much of its existence. Since the advent of computers in the
        1950s, ambitious programmers promsied we were on the cusp of building
        systems smarter than us. But time and time again, those promises fell
        flat.
      </p>
      <p>
        The core challenge of AI is the rigidity of computers. Computer chips
        themselves can only perform a handful of simple operations: things like
        adding numbers, comparing values, and moving data between memory
        locations. Programs are constructed by chaining together
        millions—sometimes billions—of these atomic operations. But for a
        program to function correctly, every step must be painstakingly spelled
        out. The programmer has to antipcate every possible case and define, in
        exact terms, what the system should do in each one.
      </p>
      <p>
        From this perspective, the dream of artificial intelligence feels almost
        absurd. Intelligence, after all, is messy—it's intuitive,
        context-dependent, and often based on experience rather than explicit
        rules. We make decisions based on gut feelings, interpret language
        filled with ambiguity, and adapt to new situations without being told
        exactly what to do. How could a system so rigid, so mechanical, ever
        give rise to the complexitity and nuance of human thought?
      </p>
      <p>
        You can probably see where this story is going. Obviously, some form of
        "machine learning" (ML) is the answer right? Well—not so much. At the
        time, scientists had no real understanding of how learning worked in the
        brain,<d-footnote
          >Which we still don't! We only have isolated insights into synaptic
          plasticity, neural circuits, and brain regions, without a unifying
          explanation of how they give rise to complex learning<d-cite
            bibtex-key="yuste2015neurons"
          ></d-cite
          ><d-cite bibtex-key="krakauer2017neuroscience"></d-cite
          ><d-cite bibtex-key="dehaene2020how"></d-cite></d-footnote
        >let alone how to replicate it in a machine.
      </p>
      <p>
        Still, there was some early optimism that it could be done. In 1957,
        Frank Rosenblatt unveiled the <i>perceptron</i>, a simple model very
        rougly analogus to neurons in the brain. Rosenblatt demonstrated how his
        machine could differntiate basic shapes on a grid, such as "T"s from
        "J"s if configured properly. Additionally, Rosenblatt proposed a simple
        learning rule that allowed his model to learn some behavior on its own
        through a handful of examples. Initially, Rossenblatt's work was met
        with excitement with <i>The New York Times</i> proclaiming that
        Rosenblatt's perceptron would be able to "walk, talk, see, write,
        reproduce itself, and be conscious of its existence".<d-cite
          bibtex-key="nyt1958perceptron"
        ></d-cite>
        Yet, that excitement soon faded as the perceptron could only learn the
        most trivial tasks. For anything more complicated, his rule would end up
        in an endless loop.<d-footnote
          >More specifically, Rosenblatt's perceptron could only learn
          linearly-separable patterns, meaning the space of inputs could be
          divided into classes using a straight line. A classic example of a
          problem it couldn't solve is the
          <a href="https://en.wikipedia.org/wiki/XOR_gate">XOR</a>
          function, famously highlighted by Marvin Minsky and Seymour Papert in
          their 1969 book <i>Perceptrons</i>.<d-cite
            bibtex-key="minsky1969perceptrons"
          ></d-cite>
        </d-footnote>
        Clearly, the system needed to be much more complicated to have any
        utility. As such, Rossenblatt tried composing networks of his
        perceptrons, but failed to find a corresponding learning rule.
      </p>
      <p>
        As such, the field of AI entered a long period of stagnation. The
        initial excitement around learning systems faded, and interest shifted
        back to rule-based approaches. In 1960, the field came tantalizingly
        close to a breakthrough. Bernard Widrow and Marcian Hoff introduced the
        Least Mean Squares (LMS) algorithm which was remarkably close to the
        modern method. But the pair stopped just short of applying their method
        for networks of perceptrons.<d-cite bibtex-key="widrow1960lms"></d-cite>
      </p>
      <p>
        The field of AI would have to wait until 1986 for this long-sought
        general learning rule.<d-footnote>Kinda</d-footnote> The method, called
        <i>backpropagation</i>, was introduced by Rumelhart, Hinton, and
        Williams and finally made it possible to train networks of
        perceptrons.<d-cite bibtex-key="rumelhart1986learning"></d-cite>.
        Backpropegation led to the development of systems for hisorically
        challenging problems like handwriting and speech recognition.<d-cite
          bibtex-key="lecun1989backpropagation"
        ></d-cite
        ><d-cite bibtex-key="bengio1992speech"></d-cite>
        While a major milestone in the field, the tasks were still narrow and
        robotic—useful, but a far cry from human intelligence. While it was
        accepted that backpropagation could solve these simpler problems, it was
        still doubtful that these methods would allow for more general
        intelligence. Surely the brain's functions couldn't be mimicked through
        a simple update rule?
      </p>
      <p>
        All of that changed in 2012. An ML system <i>AlexNet</i
        ><d-cite bibtex-key="krizhevsky2012imagenet"></d-cite> blew past the
        competition in the <i>ImageNet</i
        ><d-cite bibtex-key="deng2009imagenet"></d-cite> challenge, a
        high-profile test for image recognition. While other systems relied on
        human engineering and carefully tuned mathematical tricks, AlexNet was
        created using nothing more than backpropagation. And it didn't just
        win—it crushed the field. So what changed? Alexnet still used the same
        core ideas that researchers had know about since the 1990s. The only
        difference was scale. AlexNet was orders of magnitude larger than
        previous systems.
      </p>
      <p>
        It's hard to overstate what a watershed moment AlexNet was for AI. Once
        dismissed as unreliable, backpropagation had become not just accepted,
        but revered—almost mythologized. What kind of fantastical algorithms had
        it found that beat the competition so badly? What did backpropagation
        know that we didn't? These quesitons spurred the dawn of interptability
        efforts.
      </p>
      <p>
        Just as building AI systems once seemed impossible, interpreting them
        seemed equally daunting. Instead of a human-understable programs like
        this:
        <d-code block language="python">&#10;x=5&#10;y=3&#10;print(x+y)</d-code
        >Researchers are instead faced with programs that look like this:
        <img src="numbas.png" width="100%" />
        <figcaption>
          These are 297 of AlexNet's weights. AlexNet has 61 million weights.
          Today's systems can have upwards of 1 trillion.
        </figcaption>
      </p>
      <p>
        After numerous failed attempts to understand models like AlexNet, the
        first interptability successes came in 2020 with the launch of the
        Distill Circuits thread.<d-cite
          bibtex-key="cammarata2020circuits"
        ></d-cite>
        Researchers mannaged to find curve detection mechanisms as well as
        detectors for more complex objects like dog ears, sparking renewed
        interest in the possibility of truly understanding these systems.
      </p>
      <p>
        Yet while interptability researchers struggled to understand AlexNet, AI
        researchers raced to create more and more complicated systems. Notably,
        OpenAI's ChatGPT models garnared a huge ammount of attention from the
        general public, spurring an avalanche of investment toward even more
        powerful systems.
      </p>
      <p>
        As a result, much of the field's focus shifted toward these new language
        models. These models presented their own unique challenges for
        interpertability. To address these challenges, Anthropic launched the
        Transformer Circuits Thread<d-cite
          bibtex-key="elhage2021transformer"
        ></d-cite
        >, a kind of spiritual successor to Distill Circuits Thread, aimed at
        reverse-engineering the inner workings of language models layer by
        layer.
      </p>
      <p>
        While the large-scale deployment of language models has motivated
        serious efforts to understand them, other modalities such as music
        remain understudied. The goal of this paper is to apply these
        established interptability techniques to this new domain.
      </p>
      <h2 id="ML">A Brief Introduction to ML</h2>
      <p>
        To make this paper as self-contained as possible, I've included a brief
        introduction to machine learning. If you're already familiar with the
        basics, feel free to skip this section. The only prior knowledge assumed
        here is comfort with basic algebraic expressions and fundamental math
        concepts like functions. In this section, we'll walk through 3 ML
        probelms:
      </p>
      <ol>
        <li>Predicting house prices based on square footage</li>
        <li>Recognizing handwritten digits</li>
        <li>Building a chatbot that can answer user querries</li>
      </ol>
      <h3 id="overview">Housing model</h3>
      <p>
        The first step in tackling any machine learning problem is gathering
        data. In this case, we'll use the <i>Ames Housing Dataset</i>, which
        contains information on 2,930 homes in Ames, Iowa.<d-cite
          bibtex-key="thapa_ames_2021"
        ></d-cite>
        Heres a graph of the data:
      </p>
      <div
        id="raw-data-plot"
        style="height: 600px; max-width: 900px; margin-top: 40px"
      ></div>
      <p>
        In ML, we almost always manually design the structure of our
        model.<d-footnote
          >There are niche exceptions to this rule such as NeuroEvolution of
          Augmenting Topologies (NEAT).
        </d-footnote>
        In this case, let's go with a linear model: $price = (m)(area) + b$.
        While the structure of our model remains fixed, the behavior of our
        model can vary dramatically depending on the specific values of the
        numbers m and b. These numbers are called
        <i>parameters</i>. Experiment with different parameter values using the
        sliders below to see how they affect the model.
      </p>
      <div id="plot" style="height: 600px; max-width: 900px"></div>
      <div class="slider-container">
        <label class="label">Slope m = <span id="mValue">100</span></label>
        <input
          type="range"
          min="0"
          max="500"
          step="1"
          value="100"
          id="mSlider"
        />
        <div class="ticks">
          <div class="tick">0</div>
          <div class="tick">250</div>
          <div class="tick">500</div>
        </div>
      </div>
      <div class="slider-container">
        <label class="label">Intercept b = <span id="bValue">0</span></label>
        <input
          type="range"
          min="0"
          max="800000"
          step="500"
          value="0"
          id="bSlider"
        />
        <div class="ticks">
          <div class="tick">0</div>
          <div class="tick">400k</div>
          <div class="tick">800k</div>
        </div>
      </div>
      <p>
        To evaluate how well our model explains the data, we use a quantitative
        measure called a <i>loss function</i>. For our housing price prediction,
        the <i>Mean Squared Error</i> (MSE) is particularly appropriate. MSE
        calculates the average of the squared differences between our model's
        predictions and the actual house prices. This penalizes larger errors
        more heavily, making it suitable for price prediction. The MSE for your
        current parameter settings is <span id="mseValue">calculating...</span>.
        Lower MSE values indicate a better fit to the data. For our simple
        housing problem, we can visualize this MSE loss as a function of our two
        parameters:
      </p>
      <div
        id="loss-landscape-container"
        style="height: 600px; width: 100%; margin: 30px 0"
      ></div>
      <p>
        The ultimate goal of machine learning is to choose parameters that
        minimize this loss function. Let's explore some several approaches to
        finding these optimal parameters.
      </p>
      <h4>Global Search</h4>
      <p>
        A naïve approach is to conduct a comprehensive search across the entire
        parameter space. One way to do this is a <i>grid search</i> in which we
        check every parameter combination in a grid. For example, we may choose
        to evaluate m between 0 and 500 and b between 0 and 800000 with a 5x5
        grid. The coarseness of the grid determines how finely we divide each
        dimension—a finer grid (e.g., 100x100) gives better results but requires
        more computation (10,000 evaluations instead of just 25). Another
        approach is <i>random search</i> in which we randomly sample points from
        the parameter space, with the number of samples determining how
        thoroughly we explore. Probabilistically, as we increase the number of
        samples, we become more likely to find points near the optimum, though
        with diminishing returns—doubling the number of samples doesn't double
        our chances of finding the global optimum. Look at the example below to
        see both these methods in action.
      </p>
      <p>
        As you can see, these global search methods work quite well for our
        simple housing problem. However, this approach doesn't scale well for
        more complicated models. A grid search in which we check 10 values for
        each parameter is feasible when there is only two parameters (simply
        10^2 operations) but would be impossible for a model with 175 billion
        parameters like the original ChatGPT (10^175,000,000,000) - a number
        vastly larger than the estimated 10^80 atoms in the observable universe.
      </p>
      <p>
        Random search suffers from similar diminishing returns: in
        high-dimensional spaces, the probability of randomly sampling near the
        optimal region becomes vanishingly small. Mathematically, if the optimal
        region occupies a fraction f of each dimension's range, then in n
        dimensions, the probability of a random point falling in the optimal
        region is f^n. For example, if the optimal region is 10% of each
        dimension's range, in just 30 dimensions the probability drops to 10^-30
        - effectively zero.
      </p>
      <p>
        This "curse of dimensionality" presents a fundamental challenge: as
        dimensions increase, the volume of the parameter space grows
        exponentially. Instead of globally searching for parameter values, we
        need a way to instead gradually improve our model. We need our model to
        learn.
      </p>
      <h4>Local Search</h4>
      <p>
        With local search, we seek to make incremental progress. We start by
        randomly choosing parameter values. From there, we sprinkle points in a
        radius around our current parameter values and choose the one with the
        lowest loss. Repeating this process many times eventually leads to a
        pretty good set of parameters. Check out this animation below.
      </p>
      <p>
        This method is a far more efficient than global search methods as we
        explore only a tiny slice of parameter space. However, local search
        faces a new challenge: local minima. If there is a significant valley in
        the loss landscape, local search may take a step out and then a step
        back in getting stuck in an infinite loop. Additionally, local search
        still isn't free from the curse of dimensionality. As the number of
        dimensions increases, the volume of the "neighborhood" around our
        current point grows exponentially.
      </p>
      <p>
        It seems like we need some miracle tool that allows us to determine
        where to travel without computing all the points around us. Enter
        gradients. Our trusty compass for navigating these high-dimensional
        parameter spaces.
      </p>
      <h4>Gradient Descent</h4>
      <p>
        First off, what even is a gradient? Gradients are the higher-dimensional
        cousins of the derivative. In calculus, derivatives measure the
        "instantaneous rate of change" of a function around a point.<d-footnote
          >The phrase "instantaneous rate of change" should definitely make you
          feel uneasy. Whenever we talk about change, we are necessarily making
          a statement about two points. What does it mean for change to happen
          in an instant? The founders of calculus wrestled with this paradox.
          Their early work relied on infinitesimals—numbers of infinitely small
          size. The modern theory of calculus uses the concept of
          <a href="https://en.wikipedia.org/wiki/Limit_of_a_function">limits</a>
          to define derivatives, and infinitesimals have been relegated to what
          is called
          <a href="https://en.wikipedia.org/wiki/Nonstandard_analysis"
            >non-standard analysis</a
          >.</d-footnote
        >
      </p>
      <p>
        A classic example involves the motion of a car. If the car is at mile
        marker 10 at noon and mile marker 40 at 1 PM, its average speed over
        that hour is 30 miles per hour. But what if you're interested in how
        fast the car is going at exactly 12:30 PM?
      </p>
      <p>
        This is where the concept of an instantaneous rate of change becomes
        essential. Instead of calculating average speed over a large interval,
        we consider smaller and smaller intervals around 12:30 PM. As the time
        interval shrinks, the average rate of change approaches the
        instantaneous rate—this is what the derivative captures. In this
        instance, the derivative acts as a sort of mathematical speedometer.
      </p>
      <p>
        So how do we actually compute derivatives? Well, there's no silver
        bullet. Mathematicians have worked out the derivatives of a bunch of
        common functions. For example, the derivative of $x^2$ works out to be
        $2x$ and the derivative of $sin(x)$ works out to be $cos(x)$. To work
        out the derivative of more complicated functions such as $sin(x^2)+5x$,
        we simply combine these basic results together using a couple of basic
        rules such as the sum rule, product rule, and chain rule. The specific
        computational rules aren't important so I'll leave them out.
      </p>
      <p>
        Getting back to gradients, what should a derivative mean in higher
        dimensions? When a function depends on more than one variables, we can
        no longer talk about a single rate of change. Instead, we look at how
        the function changes as we vary each input independently. These are
        called <i>partial derivatives</i>. The <i>gradient</i> simply brings
        these partial derivatives together into a vector. As it turns out, this
        gradient vector always points in the direction of steepest ascent (this
        should make sense as steeper axes have a larger component).
      </p>
      <p>
        In computing the gradient vector, we don't need to compute each partial
        derivative independently as that leads to a ton of repeated computation.
        Instead, we can employ the aforementioned backpropagation algorithm.
        Backpropegation works by first computing running the model (whats called
        a forward pass) and then working backwards to find each partial
        derivative (the backward pass). Armed with the gradient vector, we've
        finally triumphed over the curse of dimensionality as computation grows
        linearly rather than exponentially with respect to the number of
        parameters.
      </p>
      <p>
        The process of gradient descent is very similar to local search. We
        start with a random choice of parameters. Then, we calculate the
        gradient vector of the loss function with respect to our parameters.
        Next, we simply travel in the opposite direction of this vector with
        some scaling factor, often called the <i>learning rate</i>. Again, this
        method is fallible to failling into local minima, but is much more
        efficient than the previously discussed methods. Look at the animation
        below to see how this gradient descent process works.
      </p>
      <p>
        There are a couple of improvements we can make to this method. For one,
        instead of calculating the exact loss with respect to all of our
        datapoints, it makes sense to calculate the loss for a small batch of
        our data. This approximates the gradient pretty well and the small
        ammount of error may actually be helpful in escaping local minima. These
        methods that calculate the loss for a batch are called stochastic
        gradient descent.
      </p>
      <p>
        Another optimization of gradient descent is adding a momentum term. This
        helps the model push faster into directions where its seeing big gains
        and smooth out wobbly trajectories. There are a couple different
        implementations of momentum, but the modern king is AdamW which also
        dynamically adjusts the learning rate for different parameters.
      </p>
      <p>
        And that's the state of the art for optimizing parameters. All of ML
        simply ammounts to optimizing much larger systems. So let's move on to a
        real problem: recognizing handwritten digits.
      </p>
      <h3>Digit Recognition</h3>
      
      <h2>MusicGen</h2>
    </d-article>

    <d-appendix
      ><h3>Specific Details</h3>
      <p>
        Lorem ipsum dolor sit amet consectetur adipisicing elit. Distinctio
        nostrum rerum dolorum necessitatibus aliquid aperiam suscipit illum
        beatae nihil, consectetur unde accusamus, quasi, voluptatem at
        aspernatur obcaecati eos laboriosam porro? Sapiente reiciendis optio
        sequi repellendus minima! Provident labore excepturi maiores quisquam
        autem temporibus iure sed, dignissimos non voluptas blanditiis veniam
        laboriosam fugit incidunt quidem id eveniet delectus odio. Perferendis,
        iusto! Totam consectetur iusto consequatur nesciunt adipisci
        voluptatibus esse voluptatum molestias nobis. Harum maiores minima
        assumenda, incidunt numquam earum alias voluptatibus consequuntur
        dolorum dolor optio rem, repellendus fugit magni inventore ut! In
        laboriosam necessitatibus omnis accusantium fugiat? Eligendi ea libero
        beatae, doloremque eos esse sunt voluptas laudantium voluptates dolor
        omnis reiciendis, enim, distinctio minima illo necessitatibus et sed
        voluptatum labore eaque. Accusantium omnis delectus non tempore
        molestiae quis totam aut explicabo hic quisquam vitae esse, repellendus
        cum blanditiis? Velit, magnam quisquam laudantium autem sit delectus
        possimus harum magni, veritatis commodi totam? Dolorem maxime
        necessitatibus nostrum expedita, quas reiciendis similique ullam quo
        assumenda illo architecto reprehenderit numquam sint vel obcaecati
        adipisci, aspernatur libero. Ducimus corrupti distinctio error possimus
        optio a. Id, laudantium. Nostrum saepe, alias libero architecto
        consequatur ullam corporis. Tempora reprehenderit voluptatum cupiditate
        ullam ab explicabo nostrum ut id sequi, tempore pariatur quis similique
        enim voluptate quam alias ducimus, consequuntur libero. Accusamus dolore
        voluptatem dolorem quo vero dolor eius, cumque culpa voluptates est
        aspernatur officiis magni ea quibusdam doloremque totam ipsa fugiat
        earum perspiciatis consectetur laudantium eos harum? Soluta, ad labore.
        Harum eum illum quasi atque animi quidem a recusandae! Saepe eligendi
        nisi vitae doloribus qui minus delectus deserunt aliquid hic totam,
        natus maxime voluptatum suscipit corporis error facere ipsum.
        Voluptatum? Ab incidunt ut voluptates omnis tenetur rem culpa provident?
        Repudiandae corrupti velit animi cumque architecto minima expedita
        aperiam nesciunt minus asperiores alias, ab fugit blanditiis repellat
        inventore ut similique. Illum?
      </p>
    </d-appendix>
    <d-bibliography src="bibliography.bib"></d-bibliography>
    <script src="https://cdn.plot.ly/plotly-2.27.0.min.js"></script>
    <script src="housing.js"></script>
  </body>
</html>
